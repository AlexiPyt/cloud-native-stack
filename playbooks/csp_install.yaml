- hosts: localhost
  gather_facts: yes
  vars_files:
    - csp_values.yaml
    - cnc_values.yaml
  environment:
    PATH: "{{ ansible_env.PATH }}:{{lookup('pipe', 'pwd')}}/google-cloud-sdk/bin/"
  tasks:
 
    - name: Get Helm on Linux
      shell: "curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
      when: ansible_system == 'Linux'

    - name: Get Kubectl on Linux
      become: true
      shell: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
      when: ansible_system == 'Linux'

    - name: Install jq on Linux
      become: true
      when: ansible_system == 'Linux'
      get_url:
        url: https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64
        dest: /usr/local/bin/jq
        mode: 0777

    - name: Get Helm and on Darwin
      shell: "brew install helm jq kubectl --force"
      when: ansible_system == 'Darwin'

    - name: Trim the GKE region from GKE zone
      when: installon_gke == true 
      shell: " echo {{ gke_zone }} | sed 's/.$//;s/.$//'"
      register: gke_region

    - name: GKE Set project Id
      when: installon_gke == true
      shell: "gcloud config set project {{ gke_project_id }}"

    - name: Create GKE cluster
      when: installon_gke == true
      block: 
        - name: Create GKE Cluster
          shell: "gcloud beta container --project {{ gke_project_id }} clusters create {{ gke_cluster_name }}  --cluster-version={{ gke_version }} --zone {{ gke_zone }} --release-channel 'regular' --machine-type 'n1-standard-4' --accelerator 'type=nvidia-tesla-t4,count=1' --image-type 'UBUNTU_CONTAINERD' --disk-type 'pd-standard' --disk-size '1000' --no-enable-intra-node-visibility --metadata disable-legacy-endpoints=true --max-pods-per-node '110' --num-nodes '1' --logging=SYSTEM,WORKLOAD --monitoring=SYSTEM --enable-ip-alias --no-enable-intra-node-visibility --default-max-pods-per-node '110' --no-enable-master-authorized-networks --tags=nvidia-ingress-all --project {{ gke_project_id }} --network 'projects/{{ gke_project_id }}/global/networks/{{ gke_network }}' --subnetwork 'projects/{{ gke_project_id }}/regions/{{ gke_region.stdout }}/subnetworks/{{ gke_network }}'"

        - name: Get Cluster credentials
          shell: "gcloud container clusters get-credentials {{ gke_cluster_name }} --zone {{ gke_zone }}"

        - name: create a namespace
          shell: "kubectl create ns gpu-operator"

        - name: apply resource quotas
          shell: "kubectl apply -f {{lookup('pipe', 'pwd')}}/resourcequota.yaml"

##EKS

    - name: Download EKS and AWS Binaries
      block:
        - name: Download EKS binaries
          get_url:
            url: "{{ item }}"
            dest: /tmp/
            mode: 0777
          loop:
            - https://github.com/weaveworks/eksctl/releases/download/v0.136.0/eksctl_Linux_arm64.tar.gz
            - https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip

        - name: Untar EKS binaries
          unarchive:
            src: "{{ item }}"
            dest: "/tmp/"
            remote_src: yes
          loop:
            - /tmp/eksctl_Linux_arm64.tar.gz
            - /tmp/awscli-exe-linux-aarch64.zip      
      when: "installon_eks == true and ansible_system == 'Linux' and ansible_architecture == 'aarch64'"

    - name: Download EKS and AWS Binaries
      block:
        - name: Download EKS binaries
          get_url:
            url: "{{ item }}"
            dest: /tmp/
            mode: 0777
          loop:
            - https://github.com/weaveworks/eksctl/releases/download/v0.136.0/eksctl_Linux_amd64.tar.gz
            - https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip

        - name: Untar EKS binaries
          unarchive:
            src: "{{ item }}"
            dest: /tmp/
            remote_src: yes    
          loop:
            - /tmp/eksctl_Linux_amd64.tar.gz
            - /tmp/awscli-exe-linux-x86_64.zip
      when: "installon_eks == true and ansible_system == 'Linux' and ansible_architecture == 'x86_64'"

    - name: Install EKS and AWS binaries
      shell: "{{ item }}"
      become: true
      loop:
        - /tmp/aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update
        - mv /tmp/eksctl /usr/local/bin/
      when: "installon_eks == true and ansible_system == 'Linux'"

    - name: Download EKS and AWS Binaries
      shell: "{{ item }}"
      loop:
        - wget https://github.com/weaveworks/eksctl/releases/download/v0.136.0/eksctl_Darwin_amd64.tar.gz
        - tar -xf eksctl_Darwin_amd64.tar.gz
        - cp ./eksctl /usr/local/bin/
        - rm eksctl eksctl_Darwin_amd64.tar.gz
        - pip install awscli
#        - curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
#        - sudo installer -pkg AWSCLIV2.pkg -target /
      when: "installon_eks == true and ansible_system == 'Darwin' and ansible_architecture == 'x86_64'"

    - name: AWS login
      when: installon_eks == true
      shell: "mkdir -p {{ ansible_user_dir }}/.aws/ && cp -r {{lookup('pipe', 'pwd')}}/aws_credentials {{ ansible_user_dir }}/.aws/credentials"

    - name: Create eks_cluster_config.yaml
      when: installon_eks == true
      copy:
        dest: "{{lookup('pipe', 'pwd')}}/eks_cluster_config.yaml"
        content: |
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig
          metadata:
            name: {{ eks_cluster_name }}
            region: {{ eks_region }}
            version: "{{ eks_version }}"
          nodeGroups:
          - name: gpu-workers
            ami: {{ eks_ami }}
            amiFamily: Ubuntu2004
            overrideBootstrapCommand: |
                #!/bin/bash
                source /var/lib/cloud/scripts/eksctl/bootstrap.helper.sh
                /etc/eks/bootstrap.sh ${CLUSTER_NAME} --container-runtime containerd --kubelet-extra-args "--node-labels=${NODE_LABELS} --max-pods=60"
            instanceType: {{ instance_type }}
            minSize: 1
            desiredCapacity: 1
            maxSize: 1
            volumeSize: 100
            ssh:
              allow: true
              publicKeyPath: ~/.ssh/id_rsa.pub

    - name: Create EKS cluster
      when: installon_eks == true
      shell: "eksctl create cluster -f {{lookup('pipe', 'pwd')}}/eks_cluster_config.yaml"

##AKS
    - name: set account
      shell:  "az account set --subscription {{ azure_account_name }}"
      when: installon_aks == true

    - name: install AZ cli tools
      shell: az aks install-cli
      become: true
      when: installon_aks == true

    - name: Create EKS cluster
      when: installon_aks == true 
      shell: "az aks create -g {{ azure_resource_group }} -n {{ aks_cluster_name }} -l {{ azure_location }} --enable-node-public-ip --node-count 1 --generate-ssh-keys --kubernetes-version {{ az_k8s_version }} --node-vm-size Standard_NC4as_T4_v3"

    - name: Get AKS Cluster credentials
      shell: az aks get-credentials --resource-group {{ azure_resource_group }}  --name {{ aks_cluster_name }}
      when: installon_aks == true 

    - name: get kubernetes version
      shell: kubectl version --output=json  | jq '.serverVersion.minor' | sed 's/\"//g;s/+//g'
      register: k8sversion

    - name: Add nvidia Helm repo
      shell: " {{ item }}"
      with_items:
        - helm repo add nvidia '{{ helm_repository }}' --force-update
        - helm repo update
      when: 'gpu_operator_registry_password == ""'

    - name: Add custom Helm repo
      shell: " {{ item }}"
      with_items:
        - helm repo add nvidia '{{ helm_repository }}' --force-update --username=\$oauthtoken --password='{{ gpu_operator_registry_password }}'
        - helm repo update
      when: 'gpu_operator_registry_password != ""'

    - name: Install the GPU Operator
      when: "installon_eks == true and k8sversion.stdout < '25' and enable_vgpu == false or installon_aks == true and k8sversion.stdout < '25' and enable_vgpu == false or installon_gke == true and k8sversion.stdout < '25' and enable_vgpu == false"
      shell: "helm install --version {{ gpu_operator_version }} --create-namespace --namespace gpu-operator '{{ gpu_operator_helm_chart }}' --set psp.enabled=true --generate-name"

    - name: Install the GPU Operator
      when: "installon_eks == true and k8sversion.stdout >= '25' and enable_vgpu == false or installon_aks == true and k8sversion.stdout >= '25' and enable_vgpu == false or installon_gke == true and k8sversion.stdout >= '25' and enable_vgpu == false"
      shell: "helm install --version {{ gpu_operator_version }} --create-namespace --namespace gpu-operator '{{ gpu_operator_helm_chart }}' --generate-name"

    - name: Install GPU Operator with vGPU on NVIDIA Cloud Native Stack
      when: "installon_eks == true and k8sversion.stdout < '25' and enable_vgpu == true or installon_aks == true and k8sversion.stdout < '25' and enable_vgpu == true or installon_gke == true and k8sversion.stdout < '25' and enable_vgpu == true"
      shell: "{{ item }}"
      with_items:
          - kubectl create namespace nvidia-gpu-operator
          - kubectl create configmap licensing-config -n nvidia-gpu-operator --from-file={{lookup('pipe', 'pwd')}}/gridd.conf --from-file={{lookup('pipe', 'pwd')}}/client_configuration_token.tok
          - kubectl create secret docker-registry registry-secret --docker-server='https://nvcr.io' --docker-username='{{ gpu_operator_registry_username }}' --docker-password='{{ gpu_operator_registry_password }}' --docker-email='{{ gpu_operator_registry_email }}' -n nvidia-gpu-operator
          - helm install --version {{ gpu_operator_version }} --create-namespace --namespace gpu-operator '{{ gpu_operator_helm_chart }}' --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config,psp.enabled=true --wait --generate-name

    - name: Install GPU Operator with vGPU on NVIDIA Cloud Native Stack
      when: "installon_eks == true and k8sversion.stdout >= '25' and enable_vgpu == true or installon_aks == true and k8sversion.stdout >= '25' and enable_vgpu == true or installon_gke == true and k8sversion.stdout >= '25' and enable_vgpu == true"
      shell: "{{ item }}"
      with_items:
          - kubectl create namespace nvidia-gpu-operator
          - kubectl create configmap licensing-config -n nvidia-gpu-operator --from-file={{lookup('pipe', 'pwd')}}/gridd.conf --from-file={{lookup('pipe', 'pwd')}}/client_configuration_token.tok
          - kubectl create secret docker-registry registry-secret --docker-server='https://nvcr.io' --docker-username='{{ gpu_operator_registry_username }}' --docker-password='{{ gpu_operator_registry_password }}' --docker-email='{{ gpu_operator_registry_email }}' -n nvidia-gpu-operator
          - helm install --version {{ gpu_operator_version }} --create-namespace --namespace gpu-operator '{{ gpu_operator_helm_chart }}' --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config --wait --generate-name

    - debug:
        msg: "Please run  `source google-cloud-sdk/path.bash.inc` "
      when: ansible_system == 'Linux' and installon_gke == true

    - debug:
        msg: "Please run  `source google-cloud-sdk/path.zsh.inc` "
      when: ansible_system == 'Darwin' and installon_gke == true