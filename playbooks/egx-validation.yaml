- hosts: master
  gather_facts: yes
  vars_files:
    - egx_version.yaml
  tasks:
    - name: Validate kubernetes cluster is up
      shell:  kubectl cluster-info | grep master
      register: cluster
      failed_when: cluster.rc == 1
      ignore_errors: yes

    - name: Check Operating System Version
      shell: cat /etc/os-release | grep -iw version | sed 's/VERSION=//g;s/"//g'
      register: osversion
      ignore_errors: yes

    - name: Check Docker Version
      become: true
      shell: docker version | grep -i -A2 'Server' | grep Version  | awk '{print $2}'
      register: dockerversion
      ignore_errors: yes

    - name: Check Kubernetes Version
      shell: kubectl get nodes | awk '{print $NF}' | grep -v VERSION
      register: k8sversion
      ignore_errors: yes

    - name: Check Helm Version
      shell: helm version --short | sed 's/v//g;s/\+.*//g'
      register: helmversion
      ignore_errors: yes

    - name: Check Nvidia GPU Operator Toolkit versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep toolkit | awk -F':' '{print $2}' | awk -F'-' '{print $1}' | head -n1
      register: nvtoolkit
      ignore_errors: yes

    - name: Check Nvidia K8s Device versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'k8s-device' | awk -F':' '{print $2}' | head -n1
      register: k8sdevice
      ignore_errors: yes

    - name: Check GPU Operator Nvidia Container Driver versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/driver'  | awk -F':' '{print $2}' | awk -F'-' '{print $1}' | head -n1
      register: nvcdriver
      ignore_errors: yes

    - name: Check GPU Operator Nvidia DGCM Exporter Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'k8s/dcgm-exporter'  | awk -F':' '{print $2}' | awk -F'-' '{print $2}' | head -n1 
      register: dgcm
      ignore_errors: yes

    - name: Check GPU Operator Node Feature Discovery Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'node-feature-discovery' | head -n1 |  awk -F':' '{print $2}'
      register: nodediscover
      ignore_errors: yes

    - name: Check GPU Operator GPU Feature Discovery Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/gpu-feature-discovery'  | awk -F':' '{print $2}' | head -n1
      register: gpudiscover
      ignore_errors: yes

    - name: Check Nvidia GPU Operator versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/gpu-operator'  | awk -F':' '{print $2}' | head -n1
      register: gpuoperator
      ignore_errors: yes

    - name: check master node is up and running
      shell: kubectl get nodes | grep -i ready
      register: nodeready
      failed_when: "'NotReady' in nodeready.stdout"
      ignore_errors: yes

    - name: Check all pods are running for Kubernetes
      shell: kubectl get pods --all-namespaces | egrep -iv 'Running|NAME|Completed'
      register: kubepods
      failed_when: kubepods.rc == 0
      ignore_errors: yes

    - name: validate helm installed
      shell: helm ls
      register: helmls
      failed_when: helmls.rc == 1
      ignore_errors: yes

    - name: Validate the GPU Operator pods State
      shell: kubectl get pods --all-namespaces | egrep -v 'kube-system|NAME'
      register: pods
      failed_when: pods.rc == 1
      ignore_errors: yes

    - name: Collecting Number of GPU's
      shell: "kubectl describe nodes | grep -A 6 Capacity | grep 'nvidia.com/gpu' | awk '{print $2}'"
      register: gpus
      ignore_errors: yes

    - name: Validating the nvidia-smi on Kubernetes
      shell: "timeout 60 kubectl run gpu-test --rm -t -i --restart=Never --image=nvidia/cuda:11.2.1-base --limits=nvidia.com/gpu={{ gpus.stdout }} -- nvidia-smi"
      register: smi
      ignore_errors: yes

    - name: Validating the CUDA with GPU
      shell: timeout 60 kubectl run cuda-vector-add --rm -t -i --restart=Never --image=k8s.gcr.io/cuda-vector-add:v0.1
      register: cuda
      ignore_errors: yes

    - name: Report Operating System Version of RHEL/CentOS
      when: "ansible_distribution in ['RedHat', 'CentOS']"
      ignore_errors: yes
      debug:
        msg: "RHEL/CentOS Operating System version {{ osversion.stdout }}"

    - name: Report Operating System Version of Ubuntu
      when: "ansible_distribution == 'Ubuntu'"
      ignore_errors: yes
      debug:
        msg: "Ubuntu Operating System version {{ osversion.stdout }}"

    - name: Report Docker Version
      ignore_errors: yes
      debug:
        msg: "Docker Version {{ dockerversion.stdout }}"

    - name: Report Kubernetes Version
      ignore_errors: yes
      debug:
        msg: "Kubernetes Version {{ k8sversion.stdout }}"

    - name: Report Helm Version
      ignore_errors: yes
      debug:
        msg: "Helm Version {{ helmversion.stdout }}"

    - name: Report Nvidia GPU Operator version 
      ignore_errors: yes
      debug:
        msg: "Nvidia GPU Operator versions {{ gpuoperator.stdout }}"

    - name: Report Nvidia Container Driver Version
      ignore_errors: yes
      debug:
        msg: "Nvidia Container Driver Version {{ nvcdriver.stdout }}"

    - name: Report GPU Operator NV Toolkit Driver
      ignore_errors: yes
      debug:
        msg: "NV Container Toolkit Version {{ nvtoolkit.stdout }}"

    - name: Report K8sDevice Plugin Version
      ignore_errors: yes
      debug:
        msg: "Nvidia K8s Device Plugin Version {{ k8sdevice.stdout }}"

    - name: Report Data Center GPU Manager (DCGM) Version
      ignore_errors: yes
      debug:
        msg: "Data Center GPU Manager (DCGM) Version {{ dgcm.stdout }}"
  
    - name: Report Node Feature Discovery Version
      ignore_errors: yes
      debug:
        msg: "Node Feature Discovery Version {{ nodediscover.stdout }}"

    - name: Report GPU Feature Discovery Version
      ignore_errors: yes
      debug:
        msg: "GPU Feature Discovery Version {{ gpudiscover.stdout }}"

    - name: Report GPU Operator Pods
      debug:
        msg: "{{ pods.stdout_lines }}"

    - name: Report Nvidia SMI Validation
      ignore_errors: yes
      debug:
        msg: "{{ smi.stdout_lines }}"

    - name: Report Cuda Validation
      ignore_errors: yes
      debug:
        msg: "{{ cuda.stdout_lines }}"

    - name: Status Check
      shell: echo "All tasks should be changed or ok, if it's failed or ignoring means that validation task failed."
      register: check

    - debug:
        msg: "{{ check.stdout }}"
