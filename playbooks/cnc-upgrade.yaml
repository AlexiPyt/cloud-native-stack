- hosts: master
  vars_files:
    - cnc_values.yaml
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
  tasks:

   - debug:
       msg: "Not a Valid Upgrade please upgrade Ubuntu to 22.04 and retry"
     when: cnc_version == 6.3
     failed_when: "cnc_version == 6.3 and ansible_distribution_major_version <= '20'"

   - name: Validate whether Kubernetes cluster installed
     shell: kubectl cluster-info
     register: k8sup
     no_log: True
     failed_when: false

   - name: Check Current Running Cloud Native Stack Version
     shell: kubectl version --short=true --client=false | grep -i server | awk '{print $NF}'
     register: k8sversion

   - when: "k8sversion.stdout == 'v1.25.2' and ansible_distribution_major_version == '22'"
     debug:
       msg: "Current Cloud Native Stack Version 8.0 "

   - when: "k8sversion.stdout == 'v1.24.6' and ansible_distribution_major_version == '22'"
     debug:
       msg: "Current Cloud Native Stack Version 7.1 "

   - when: "k8sversion.stdout == 'v1.24.2' and ansible_distribution_major_version == '22'"
     debug:
       msg: "Current Cloud Native Stack Version 7.0 "

   - when: "k8sversion.stdout == 'v1.23.12'"
     debug:
       msg: "Current Cloud Native Stack Version 6.3"

   - when: "k8sversion.stdout == 'v1.23.8'"
     debug:
       msg: "Current Cloud Native Stack Version 6.2"

   - when: "k8sversion.stdout == 'v1.23.5'"
     debug:
       msg: "Current Cloud Native Stack Version 6.1"

   - when: "k8sversion.stdout == 'v1.23.2'"
     debug:
       msg: "Current Cloud Native Stack Version 6.0"

   - when: "k8sversion.stdout == 'v1.22.5'"
     debug:
       msg: "Current Cloud Native Stack Version 5.2"

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.22.5'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 6.0

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.23.2'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 6.1

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.23.5'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 6.2

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.23.12' and ansible_distribution_major_version == '22'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 7.0

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.23.8'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 6.3

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.24.2' and ansible_distribution_major_version == '22'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 7.1

   - name: Create Cloud Native Stack cnc_version.yaml
     when: "k8sversion.stdout == 'v1.24.6' and ansible_distribution_major_version == '22'"
     copy:
       dest: "/tmp/cnc_version.yaml"
       content: |
         cnc_version: 8.0

   - name: Fetch cnc_version.yaml
     ansible.builtin.fetch:
       src: "/tmp/cnc_version.yaml"
       dest: "{{lookup('pipe', 'pwd')}}/cnc_version.yaml"
       flat: yes

   - name: Update the cnc_values
     when: cnc_docker == true and cnc_nvidia_driver == true
     replace:
       path: "{{lookup('pipe', 'pwd')}}/cnc_values.yaml"
       regexp: '{{ item.regex }}'
       replace: '{{ item.replace }}'
     with_items:
       - { regex: 'cnc_docker: no', replace: 'cnc_docker: yes' }
       - { regex: 'cnc_nvidia_driver: no', replace: 'cnc_nvidia_driver: yes' }

   - name: Update the cnc version in cnc_values.yaml
     shell: |
       version=$(cat {{lookup('pipe', 'pwd')}}/cnc_version.yaml | awk -F':' '{print $2}' | head -n1 | tr -d ' ' | tr -d '\n\r');  driver_version=$(cat {{lookup('pipe', 'pwd')}}/cnc_values_$version.yaml | grep driver_version | awk -F':' '{print $2}' | head -n1 | tr -d ' ' | tr -d '\n\r')
       sed -i "s/cnc_version: .*/cnc_version: $version/g; s/gpu_driver_version: .*/gpu_driver_version: $driver_version/g" {{lookup('pipe', 'pwd')}}/cnc_values.yaml

- import_playbook: prerequisites.yaml

- hosts: all
  become: true
  vars_files:
    - cnc_values.yaml
  become_method: sudo
  tasks:
    - name: Update Containerd Runtime for NVIDIA Cloud Native Stack
      become: true
      block:
        - name: Write defaults to config.toml
          get_url:
            dest: /etc/containerd/config.toml
            url: https://raw.githubusercontent.com/NVIDIA/cloud-native-stack/master/playbooks/config.toml
            mode: 0664

        - name: Enable systemd cgroups
          shell: sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

        - name: restart containerd
          service:
            name: containerd
            state: restarted
            daemon_reload: yes
      when: "cnc_docker == true and ansible_distribution == 'Ubuntu' and ansible_distribution_major_version >= '20'"

    - name: Check NVIDIA Driver Modules are loaded
      shell: "lsmod | grep -i nvidia"
      register: nvidia_mod
      no_log: True
      failed_when: false

    - name: Check NVIDIA SMI loaded
      shell: "nvidia-smi"
      register: nvidia_smi
      no_log: True
      failed_when: false

    - name: NVIDIA Driver Clean Up
      when:  cnc_version >= 6.1 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0
      block:
        - name: Remove Ubuntu unattended upgrades to prevent apt lock
          ansible.builtin.apt:
            name: unattended-upgrades
            state: absent
            purge: yes
          register: apt_cleanup
          retries: 10
          until: apt_cleanup is success

        - name: Remove OLD Apt Repository
          apt_repository:
            repo: ppa:graphics-drivers/ppa
            state: absent
          register: ppa_clean
          retries: 10
          until: ppa_clean is success

        - name: Remove NVIDIA packages
          apt:
            name:
            - "*cuda*"
            - "libnvidia-cfg1-*"
            - "libnvidia-common-*"
            - "libnvidia-compute-*"
            - "libnvidia-decode-*"
            - "libnvidia-encode-*"
            - "libnvidia-extra-*"
            - "libnvidia-fbc1-*"
            - "libnvidia-gl-*"
            - "nvidia-compute-utils-*"
            - "nvidia-dkms-*"
            - "nvidia-driver-*"
            - "nvidia-kernel-common-*"
            - "nvidia-kernel-source-*"
            - "nvidia-modprobe"
            - "nvidia-prime"
            - "nvidia-settings"
            - "nvidia-utils-*"
            - "screen-resolution-extra"
            - "xserver-xorg-video-nvidia-*"
            - "gdm*"
            - "xserver-xorg-*"
            autoremove: yes
            purge: yes
            state: absent
          register: nvidia_cleanup
          retries: 10
          until: nvidia_cleanup is success

        - name: Remove old keyring
          shell:
            cmd: "apt-key del 7fa2af80"


    - name: Add CUDA APT Key
      become: true
      when:  "cnc_version == 6.3 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'x86_64'"
      apt:
        deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
        state: "present"
      register: cuda_ring
      retries: 10
      until: cuda_ring is success

    - name: Add CUDA APT Key
      become: true
      when:  "cnc_version == 7.1 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'x86_64' and ansible_distribution_major_version >= '22' or cnc_version == 8.0 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'x86_64' and ansible_distribution_major_version >= '22'"
      apt:
        deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
        state: "present"
      register: cuda_ring
      retries: 10
      until: cuda_ring is success

    - name: Add CUDA APT Key
      become: true
      when:  "cnc_version == 6.3 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'aarch64' and ansible_distribution_major_version == '20' "
      apt:
        deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/sbsa/cuda-keyring_1.0-1_all.deb
        state: "present"
      register: cuda_ring
      retries: 10
      until: cuda_ring is success

    - name: Install NVIDIA TRD Driver
      become: true
      when:  "cnc_version == 7.1 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'aarch64' and ansible_distribution_major_version >= '22' or cnc_version == 8.0 and nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and ansible_architecture == 'aarch64' and ansible_distribution_major_version >= '22' "
      apt:
        deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.0-1_all.deb
        state: "present"
      register: cuda_ring
      retries: 10
      until: cuda_ring is success

    - name: Install NVIDIA TRD Driver
      become: true
      when:  "nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and cnc_version == 6.3 or nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and cnc_version == 7.1 or nvidia_mod.rc == 0 and nvidia_smi.rc == 0 and cnc_version == 8.0"
      ignore_errors: true
      block:
        - name: Force an apt update
          apt:
            update_cache: true
          changed_when: false
          register: update
          retries: 10
          until: update is success

        - name: Ensure kmod is installed
          apt:
            name: "kmod"
            state: "present"
          register: kmod_check
          retries: 10
          until: kmod_check is success

        - name: Temporarily adjust account password policy
          shell: chage -d 1 root

        - name: Install driver packages
          apt:
            name: cuda-11.8
            state: present
          register: cuda_install
          retries: 10
          until: cuda_install is success

        - name: Setup root account password policy
          shell: chage -d 0 root

- hosts: master
  vars_files:
    - cnc_values.yaml
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
  tasks:
   - name: Validate whether Kubernetes cluster installed
     shell: kubectl cluster-info
     register: k8sup
     no_log: True
     failed_when: false

   - name: Check Current Running Cloud Native Stack Version
     shell: kubectl version --short=true --client=false | grep -i server | awk '{print $NF}'
     register: k8sversion

   - name: Upgrade the Cloud Native Stack from 7.1 to 8.0
     shell: kubeadm upgrade apply v1.25.2 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and ansible_distribution_major_version == '22'"
     become: true

   - name: Upgrade the Cloud Native Stack from 7.0 to 7.1
     shell: kubeadm upgrade apply v1.24.6 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and ansible_distribution_major_version == '22'"
     become: true

   - name: Upgrade the Cloud Native Stack from 6.3 to 7.0
     shell: kubeadm upgrade apply v1.24.2 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.12' and ansible_distribution_major_version == '22'"
     become: true

   - name: Upgrade the Cloud Native Stack from 6.2 to 6.3
     shell: kubeadm upgrade apply v1.23.12 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8'"
     become: true

   - name: Upgrade the Cloud Native Stack from 6.1 to 6.2
     shell: kubeadm upgrade apply v1.23.8 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.5'"
     become: true

   - name: Upgrade the Cloud Native Stack from 6.0 to 6.1
     shell: kubeadm upgrade apply v1.23.5 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.2'"
     become: true

   - name: Upgrade the Cloud Native Stack from 5.2 to 6.0
     shell: kubeadm upgrade apply v1.23.5 --force
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.2'"
     become: true

   - name: Install networking plugin to kubernetes cluster on NVIDIA Cloud Native Stack 7.1 or 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' or 'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6'"
     command: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml

   - name: Install networking plugin to kubernetes cluster on NVIDIA Cloud Native Stack 7.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.12' and ansible_distribution_major_version == '22'"
     command: kubectl apply -f https://projectcalico.docs.tigera.io/archive/v3.23/manifests/calico.yaml

   - name: Install networking plugin to kubernetes cluster on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8'"
     command: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml

   - name: Install networking plugin to kubernetes cluster on NVIDIA Cloud Native Stack 6.2
     when: "'running' in k8sup.stdout and cnc_version <= 6.1"
     command: kubectl apply -f https://projectcalico.docs.tigera.io/archive/v3.21/manifests/calico.yaml

   - name: Update Network plugin for Calico on NVIDIA Cloud Native Stack > 3.1
     when: "'running' in k8sup.stdout and cnc_version >= 3.1 "
     shell: "sleep 5; kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=ens*,eth*,enc*,bond*,enp*,eno*"

   - name: Taint the Kubernetes Control Plane node
     when: "'running' in k8sup.stdout and cnc_version < 7.0"
     command: kubectl taint nodes --all node-role.kubernetes.io/master-

   - name: Taint the Kubernetes Control Plane node
     when: "'running' in k8sup.stdout and cnc_version == 7.0 or 'running' not in k8sup.stdout and cnc_version == 7.1"
     command: kubectl taint nodes --all node-role.kubernetes.io/master- node-role.kubernetes.io/control-plane-

   - name: Taint the Kubernetes Control Plane node
     when: "'running' in k8sup.stdout and cnc_version == 8.0"
     command: kubectl taint nodes --all node-role.kubernetes.io/control-plane-

   - name: Uninstall the GPU Operator with MIG
     shell: |
       kubectl label nodes --all nvidia.com/mig.config=all-disabled --overwrite
       sleep 5
       config_state=$(kubectl describe nodes  |grep mig.config.state |head -n1 | awk -F'=' '{print $2}')
       while [ $config_state != "success" ]
       do
         sleep 5
         config_state=$(kubectl describe nodes  |grep mig.config.state | head -n1 |awk -F'=' '{print $2}')
       done
     when: "enable_mig == true and cnc_version >= 4.1"
     async: 120
     args:
       executable: /bin/bash

   - name: Upgrade GPU Operator on Cloud Native Stack 6.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.2' and cnc_docker == false"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com
       - kubectl apply -f https://raw.githubusercontent.com/NVIDIA/gpu-operator/release-1.10/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
       - helm show --version=v1.10.1 values nvidia/gpu-operator > /tmp/values.yaml
       - helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 1.10.1 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version='{{ gpu_driver_version }}'

   - name: Upgrade GPU Operator on Cloud Native Stack 6.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.5' and cnc_docker == false"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com
       - kubectl apply -f https://raw.githubusercontent.com/NVIDIA/gpu-operator/release-1.11/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
       - helm show --version=v1.11.0 values nvidia/gpu-operator > /tmp/values.yaml
       - helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 1.11.0 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version='{{ gpu_driver_version }}'

   - name: Upgrade the GPU Operator values on Cloud Native Stack 6.3 or 7.1 or 8.0
     when: "k8sversion.stdout == 'v1.23.8' or k8sversion.stdout == 'v1.24.2' or k8sversion.stdout == 'v1.24.6'"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com
       - kubectl apply -f https://raw.githubusercontent.com/NVIDIA/gpu-operator/release-22.09/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
       - helm show --version=v22.09 values nvidia/gpu-operator > /tmp/values.yaml

   - name: Upgrade GPU Operator on Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the Signed GPU Operator on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version=520-signed

   - name: Upgrading the GPU Operator with vGPU on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == false and enable_vgpu == true and enable_rdma == false and enable_gds == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config

   - name: Upgrading the GPU Operator with MIG on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == true and  enable_rdma == false and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and GDS with Host MOFEDon NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrade GPU Operator on Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false"
     shell: helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the Signed GPU Operator on NVIDIA Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version=520-signed

   - name: Upgrading the GPU Operator with vGPU on NVIDIA Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == false and enable_vgpu == true and enable_rdma == false and enable_gds == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config

   - name: Upgrading the GPU Operator with MIG on NVIDIA Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == true and  enable_rdma == false and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 7.1
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and GDS with Host MOFEDon NVIDIA Cloud Native Stack 7.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrade GPU Operator on Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false"
     shell: helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the Signed GPU Operator on NVIDIA Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.version=520-signed

   - name: Upgrading the GPU Operator with vGPU on NVIDIA Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == false and enable_vgpu == true and enable_rdma == false and enable_gds == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config

   - name: Upgrading the GPU Operator with MIG on NVIDIA Cloud Native Stack 6.3
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == true and  enable_rdma == false and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrading the GPU Operator with RDMA and GDS with Host MOFEDon NVIDIA Cloud Native Stack 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --values /tmp/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}'

   - name: Upgrade GPU Operator on Cloud Native Stack 6.3 or 7.1 or 8.0
     when: "'running' in k8sup.stdout and k8sversion.stdout == 'v1.23.8' and cnc_docker == true or 'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.2' and cnc_docker == true or 'running' in k8sup.stdout and k8sversion.stdout == 'v1.24.6' and cnc_docker == true"
     shell: "helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version 22.09 --namespace nvidia-gpu-operator --set mig.strategy=single,driver.enabled=false,toolkit.enabled=false --values /tmp/values.yaml"

   - name: Enable MIG profile with GPU Operator on NVIDIA Cloud Native Stack
     when: "enable_mig == true and enable_vgpu == false and cnc_version >= 4.1"
     shell: "kubectl label nodes --all nvidia.com/mig.config={{ mig_profile }} --overwrite"
     args:
       warn: false

   - name: GPU Operator Changes to the ARM system for Cloud Native Stack 6.3
     shell: kubectl patch ds/nvidia-driver-daemonset -n nvidia-gpu-operator -p '{"spec":{"template":{"spec":{"containers":[{"name":"nvidia-driver-ctr","image":"nvcr.io/nvidia/driver:515.65.01-ubuntu20.04"}]}}}}'
     when: "cnc_version == 6.3 and ansible_architecture == 'aarch64'"

   - name: GPU Operator Changes to the ARM system for Cloud Native Stack 7.1 or 8.0
     shell: kubectl patch ds/nvidia-driver-daemonset -n nvidia-gpu-operator -p '{"spec":{"template":{"spec":{"containers":[{"name":"nvidia-driver-ctr","image":"nvcr.io/nvidia/driver:515.65.01-ubuntu22.04"}]}}}}'
     when: "cnc_version >= 7.1 and ansible_architecture == 'aarch64'"

   - name: Reboot the system
     become: true
     when:  cnc_docker == true and cnc_nvidia_driver == true
     reboot:
       reboot_timeout: 900